{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "artistic-sudan",
   "metadata": {
    "id": "artistic-sudan"
   },
   "source": [
    "# Lab 6 - Pre-processing large data with PySpark\n",
    "# TOC -\n",
    "\n",
    "- Column functions\n",
    "- Working with parquet files\n",
    "\t- Parquet vs csv\n",
    "\t- reading data from parquet files\n",
    "- dealing with missing\n",
    "    - detect missing, count and percentage\n",
    "\t- replace null entries\n",
    "\t\t- uni variate example\n",
    "\t- drop null entries\n",
    "- drop duplicates\n",
    "- save the clean df to a new parquet file.\n",
    "- Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdc47ba-7823-4ac8-aed8-904381b29ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486d2e16-847e-461c-b716-bf1a999cd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Spark SQL Course\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "basic-tablet",
   "metadata": {
    "id": "basic-tablet"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/11 08:13:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## start the session\n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Lab6\").getOrCreate()\n",
    "# spark context to interact with the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-guinea",
   "metadata": {
    "id": "dynamic-guinea"
   },
   "source": [
    "## Column Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-nigeria",
   "metadata": {
    "id": "honey-nigeria"
   },
   "source": [
    "### Numeric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "genetic-lecture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:07:57.758615Z",
     "start_time": "2023-11-25T10:07:57.714893Z"
    },
    "id": "genetic-lecture"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "columns = [\"brand\", \"cost\"]\n",
    "df = spark.createDataFrame([\n",
    "    (\"garnier\", 3.49),\n",
    "    (\"elseve\", 2.71)\n",
    "], columns)\n",
    "\n",
    "round_cost = fn.round(df.cost, 1)\n",
    "floor_cost = fn.floor(df.cost)\n",
    "ceil_cost = fn.ceil(df.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4acacc15-b161-4c8b-acae-f8410a0c5594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|  brand|cost|\n",
      "+-------+----+\n",
      "|garnier|3.49|\n",
      "| elseve|2.71|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "julian-height",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:04:21.663603Z",
     "start_time": "2023-11-25T10:04:21.250563Z"
    },
    "id": "julian-height",
    "outputId": "aa40f01b-ac4a-44eb-b9c6-d83e4bcfa000",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+----+\n",
      "|  brand|cost|round|floor|ceil|\n",
      "+-------+----+-----+-----+----+\n",
      "|garnier|3.49|  3.5|    3|   4|\n",
      "| elseve|2.71|  2.7|    2|   3|\n",
      "+-------+----+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('round', fn.round(df.cost, 1))\\\n",
    "    .withColumn('floor', floor_cost)\\\n",
    "    .withColumn('ceil', ceil_cost)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "516f958c-4bd9-4fcb-b3ed-c8c7418581b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|  brand|cost|\n",
      "+-------+----+\n",
      "|garnier|3.49|\n",
      "| elseve|2.71|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-seller",
   "metadata": {
    "id": "phantom-seller"
   },
   "source": [
    "**Important** :Notice How the original dataframe did not change? you have to assign the changes to a new df as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-sperm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:08:33.512267Z",
     "start_time": "2023-11-25T10:08:33.490392Z"
    },
    "id": "nervous-sperm"
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('round', round_cost)\\\n",
    "    .withColumn('floor', floor_cost)\\\n",
    "    .withColumn('ceil', ceil_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-confidence",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:08:34.343434Z",
     "start_time": "2023-11-25T10:08:34.067765Z"
    },
    "id": "south-confidence",
    "outputId": "2934d63e-e0a8-4bf7-f619-de2aa8e2d161",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+-----+----+\n",
      "|  brand|cost|round|floor|ceil|\n",
      "+-------+----+-----+-----+----+\n",
      "|garnier|3.49|  3.5|    3|   4|\n",
      "| elseve|2.71|  2.7|    2|   3|\n",
      "+-------+----+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-midwest",
   "metadata": {
    "id": "outer-midwest"
   },
   "source": [
    "### Datetime functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-wages",
   "metadata": {
    "id": "necessary-wages"
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (date(2015, 1, 1), date(2015, 1, 15)),\n",
    "    (date(2015, 2, 21), date(2015, 3, 8)),\n",
    "], [\"start_date\", \"end_date\"])\n",
    "\n",
    "days_between = fn.datediff(df.end_date, df.start_date)\n",
    "start_month = fn.month(df.start_date)\n",
    "\n",
    "df.withColumn('days_between', days_between)\\\n",
    "    .withColumn('start_month', start_month)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-carroll",
   "metadata": {
    "id": "noticed-carroll"
   },
   "source": [
    "### User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focal-reminder",
   "metadata": {
    "id": "focal-reminder"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+\n",
      "|first|second|               udf|\n",
      "+-----+------+------------------+\n",
      "|    1|     3|3 is bigger than 1|\n",
      "|    4|     2|4 is bigger than 2|\n",
      "+-----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n",
    "\n",
    "def my_func(col_1, col_2):\n",
    "    if (col_1 > col_2):\n",
    "        return \"{} is bigger than {}\".format(col_1, col_2)\n",
    "    else:\n",
    "        return \"{} is bigger than {}\".format(col_2, col_1)\n",
    "\n",
    "my_udf = fn.udf(my_func, StringType())\n",
    "\n",
    "df.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-sampling",
   "metadata": {
    "id": "mounted-sampling"
   },
   "source": [
    "**EXTREMELY IMPORTANT note on UDFs**:\n",
    "\n",
    "Creating UDFs should be a last resort. Only write a UDF if you are certain tha the functionallity you to implement cannot be done using spark functions which is usually not the case. Writing UDFs is exteremly risky and can be too costly as the code is not optimised unlike the spark functions/API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-friend",
   "metadata": {
    "id": "dated-friend"
   },
   "source": [
    "## Working with Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-deployment",
   "metadata": {
    "id": "induced-deployment"
   },
   "source": [
    "what is parquet and its advantages [source](https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-ownership",
   "metadata": {
    "id": "fancy-ownership"
   },
   "source": [
    "What is Parquet File?\n",
    "\n",
    "Apache Parquet file is a columnar storage format available to any project in the Hadoop ecosystem, regardless of the choice of data processing framework, data model, or programming language.\n",
    "Advantages:\n",
    "\n",
    "While querying columnar storage, it skips the nonrelevant data very quickly, making faster query execution. As a result aggregation queries consume less time compared to row-oriented databases.\n",
    "\n",
    "It is able to support advanced nested data structures.\n",
    "\n",
    "Parquet supports efficient compression options and encoding schemes.\n",
    "\n",
    "Pyspark SQL provides support for both reading and writing Parquet files that automatically capture the schema of the original data, It also reduces data storage by 75% on average. Pyspark by default supports Parquet in its library hence we donâ€™t need to add any dependency libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-legislature",
   "metadata": {
    "id": "verified-legislature"
   },
   "source": [
    "### Reading Data from Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62a106a7-2e16-4785-9243-554128ee51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32m'Lab 4 - Spark Intro.ipynb'\u001b[0m*       \u001b[01;32mgreen_tripdata_2015-07.parquet\u001b[0m*\n",
      " \u001b[01;32mLab5_Spark_II.ipynb\u001b[0m*              hello_spark.ipynb\n",
      " \u001b[01;32mLab_6.ipynb\u001b[0m*                     \u001b[01;32m'lab 6 - Cleaning and partitioning.ipynb'\u001b[0m*\n",
      " Writing_to_iceberg.ipynb          \u001b[01;32mnetflix-titles.csv\u001b[0m*\n",
      " \u001b[01;32mbaby_names_unclean_new.parquet\u001b[0m*   writing_to_hdfs.ipynb\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cordless-grill",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:22:05.949142Z",
     "start_time": "2023-11-25T10:22:05.522042Z"
    },
    "id": "cordless-grill"
   },
   "outputs": [],
   "source": [
    "df_names = spark.read.parquet('hdfs://namenode-master/baby_names_unclean_new.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0dd4f526-2089-4884-af72-b06c7e9f3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names_t = spark.read.parquet('hdfs://namenode-master/baby_names_postlab_byy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "408e3b08-d343-43f6-81dc-8c9cc98adf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names_t.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "institutional-alaska",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:22:06.822292Z",
     "start_time": "2023-11-25T10:22:06.818652Z"
    },
    "id": "institutional-alaska",
    "outputId": "483998f6-84ba-4727-ed8c-33129203b8aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- n: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-spice",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:22:23.145817Z",
     "start_time": "2023-11-25T10:22:23.042339Z"
    },
    "id": "durable-spice",
    "outputId": "62a7db2a-d72b-4bdf-9667-dc868c5b2584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---+------+\n",
      "|      name|    n|sex|  year|\n",
      "+----------+-----+---+------+\n",
      "|    Emilia|112.0|  F|1985.0|\n",
      "|     Kelsi|112.0|  F|1985.0|\n",
      "|    Margot|112.0|  F|1985.0|\n",
      "|    Mariam|112.0|  F|1985.0|\n",
      "|  Scarlett|112.0|  F|1985.0|\n",
      "|      Aida|111.0|  F|1985.0|\n",
      "|    Ashlei|111.0|  F|1985.0|\n",
      "|     Greta|111.0|  F|1985.0|\n",
      "|    Jaimee|111.0|  F|1985.0|\n",
      "|     Lorna|111.0|  F|1985.0|\n",
      "|   Rosario|111.0|  F|1985.0|\n",
      "|     Sandi|111.0|  F|1985.0|\n",
      "|   Sharina|111.0|  F|1985.0|\n",
      "|    Tashia|111.0|  F|1985.0|\n",
      "|     Adina|110.0|  F|1985.0|\n",
      "|    Ahsley|110.0|  F|1985.0|\n",
      "|Alessandra|110.0|  F|1985.0|\n",
      "|    Amalia|110.0|  F|1985.0|\n",
      "|    Chelsi|110.0|  F|1985.0|\n",
      "|    Darcie|110.0|  F|1985.0|\n",
      "+----------+-----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-browser",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:23:05.367735Z",
     "start_time": "2023-11-25T10:23:05.267240Z"
    },
    "collapsed": true,
    "id": "amateur-browser",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7b26f952-16aa-49a2-b1bb-05277c61ed12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------\n",
      " name | Emilia     \n",
      " n    | 112.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 1----------\n",
      " name | Kelsi      \n",
      " n    | 112.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 2----------\n",
      " name | Margot     \n",
      " n    | 112.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 3----------\n",
      " name | Mariam     \n",
      " n    | 112.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 4----------\n",
      " name | Scarlett   \n",
      " n    | 112.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 5----------\n",
      " name | Aida       \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 6----------\n",
      " name | Ashlei     \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 7----------\n",
      " name | Greta      \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 8----------\n",
      " name | Jaimee     \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 9----------\n",
      " name | Lorna      \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 10---------\n",
      " name | Rosario    \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 11---------\n",
      " name | Sandi      \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 12---------\n",
      " name | Sharina    \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 13---------\n",
      " name | Tashia     \n",
      " n    | 111.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 14---------\n",
      " name | Adina      \n",
      " n    | 110.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 15---------\n",
      " name | Ahsley     \n",
      " n    | 110.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 16---------\n",
      " name | Alessandra \n",
      " n    | 110.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 17---------\n",
      " name | Amalia     \n",
      " n    | 110.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 18---------\n",
      " name | Chelsi     \n",
      " n    | 110.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "-RECORD 19---------\n",
      " name | Darcie     \n",
      " n    | 110.0      \n",
      " sex  | F          \n",
      " year | 1985.0     \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## useful if there are many features, view each record as a column\n",
    "df_names.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-shield",
   "metadata": {
    "id": "serial-shield"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naughty-three",
   "metadata": {
    "id": "naughty-three"
   },
   "source": [
    "Summary of df. similar to info. however note that this is a costly operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66aa09c0-8bf1-4400-88be-16ac188c622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names_part32 = df_names.repartition(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56938ffd-68da-4e8c-b8a2-cbf638337c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names_part32.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "hispanic-arlington",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:24:29.546876Z",
     "start_time": "2023-11-25T10:24:08.258297Z"
    },
    "id": "hispanic-arlington"
   },
   "outputs": [],
   "source": [
    "# summary of the df\n",
    "df_summary = df_names_part32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "printable-perspective",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:25:11.447505Z",
     "start_time": "2023-11-25T10:25:11.443456Z"
    },
    "id": "printable-perspective",
    "outputId": "a2a29b2d-013e-43ee-f96d-e656bcc15ae9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- n: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_summary.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "representative-median",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:25:24.914978Z",
     "start_time": "2023-11-25T10:25:24.882129Z"
    },
    "id": "representative-median",
    "outputId": "d427d94a-d3df-4835-b33e-b9867b629807",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-------+------------------+\n",
      "|summary|   name|                 n|    sex|              year|\n",
      "+-------+-------+------------------+-------+------------------+\n",
      "|  count|1923793|           1923747|1923712|           1924665|\n",
      "|   mean|    NaN|180.86667373620335|   NULL|1974.8509943288832|\n",
      "| stddev|    NaN|1533.3759020310704|   NULL| 34.02947955836609|\n",
      "|    min|  Aaban|               5.0|      F|            1880.0|\n",
      "|    25%|    NaN|               7.0|   NULL|            1951.0|\n",
      "|    50%|    NaN|              12.0|   NULL|            1985.0|\n",
      "|    75%|    NaN|              32.0|   NULL|            2003.0|\n",
      "|    max|  Zzyzx|           99686.0|      M|            2017.0|\n",
      "+-------+-------+------------------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-weapon",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:26:22.616566Z",
     "start_time": "2023-11-25T10:26:22.578454Z"
    },
    "id": "operating-weapon",
    "outputId": "0b6005f4-e051-419c-e495-6d6c77a99de8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|   name|\n",
      "+-------+-------+\n",
      "|  count|1923793|\n",
      "|   mean|    NaN|\n",
      "| stddev|    NaN|\n",
      "|    min|  Aaban|\n",
      "|    25%|    NaN|\n",
      "|    50%|    NaN|\n",
      "|    75%|    NaN|\n",
      "|    max|  Zzyzx|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## selectiing specific columns\n",
    "df_summary.select('summary','name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-intranet",
   "metadata": {
    "id": "municipal-intranet"
   },
   "source": [
    "## Dealing with Missing Data\n",
    "\n",
    "### Detect Missing Data, Count, and Percentage\n",
    "\n",
    "To detect missing values we will use the fucniton isNull from the column class under the spark.sql.fucntions\n",
    "\n",
    "Syntax of isNull() -> column_name.isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "behind-bundle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:36:25.959141Z",
     "start_time": "2023-11-25T10:36:25.956658Z"
    },
    "id": "behind-bundle"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-toyota",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:39:05.624689Z",
     "start_time": "2023-11-25T10:39:05.330874Z"
    },
    "id": "foster-toyota"
   },
   "outputs": [],
   "source": [
    "names_null_count = df_names.filter(df_names.name.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-synthetic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:39:46.420857Z",
     "start_time": "2023-11-25T10:39:46.305476Z"
    },
    "id": "palestinian-synthetic"
   },
   "outputs": [],
   "source": [
    "len_name = df_names.count()\n",
    "perc = (names_null_count / len_name) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-photography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:39:50.181151Z",
     "start_time": "2023-11-25T10:39:50.177520Z"
    },
    "id": "written-photography",
    "outputId": "1117837c-ee78-42b8-9c77-3134df059aad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.045306585821428665"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-spokesman",
   "metadata": {
    "id": "bearing-spokesman"
   },
   "source": [
    "### Replace Null Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-convertible",
   "metadata": {
    "id": "destroyed-convertible"
   },
   "source": [
    "#### Univariate Example\n",
    "Replace missing names with it mode.\n",
    "There is no mode funciton that can be used directly on a column similar to min,max and so on.\n",
    "\n",
    "So what we have to do is groupby the target column,sort by count desc and limit 1. Just like writing a query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "beautiful-evolution",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T10:55:22.779458Z",
     "start_time": "2023-11-25T10:55:22.721762Z"
    },
    "id": "beautiful-evolution"
   },
   "outputs": [],
   "source": [
    "mode_df = df_names.groupBy('name').count().orderBy(col('count').desc()).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "retired-gauge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:07:04.690558Z",
     "start_time": "2023-11-25T11:07:03.536195Z"
    },
    "id": "retired-gauge",
    "outputId": "20ac5f36-7851-4510-e9ce-21135d38b045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|count|\n",
      "+----+-----+\n",
      "|NULL|  872|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-broadcasting",
   "metadata": {
    "id": "bacterial-broadcasting"
   },
   "source": [
    "Notice how this return a df. to access the value of count we will access the column count, then extract the column using collect, which returns an rdd. we will access the first row, and its first element to extract the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "suitable-sally",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:06:16.835669Z",
     "start_time": "2023-11-25T11:06:15.904764Z"
    },
    "id": "suitable-sally",
    "outputId": "8e2ed5aa-03bf-4cfb-8ebd-27cb5922c972",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "872"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_df.select('count').collect()[0][0]\n",
    "# [0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-tender",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:21:41.506038Z",
     "start_time": "2023-11-25T11:21:41.503919Z"
    },
    "id": "pointed-tender"
   },
   "outputs": [],
   "source": [
    "## Note that because of the nature of this of dataset there is a column called n\n",
    "## Which represents how many times that name was given to a baby in a given year.\n",
    "# The code above was mainly for demonstaration\n",
    "# You can try out your self to figure how to write a query to extract the most common name across all years.\n",
    "# for now we will just impute the missing values in sex using the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cardiac-miami",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:28:15.654987Z",
     "start_time": "2023-11-25T11:28:15.631930Z"
    },
    "id": "cardiac-miami"
   },
   "outputs": [],
   "source": [
    "mode_gender = df_names.groupBy('sex').count().orderBy(col('count').desc()).limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ecd139d-5cbd-426a-a9bd-eb738d404a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|sex|  count|\n",
      "+---+-------+\n",
      "|  F|1137711|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mode_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "standard-reconstruction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:28:25.587515Z",
     "start_time": "2023-11-25T11:28:25.088003Z"
    },
    "id": "standard-reconstruction"
   },
   "outputs": [],
   "source": [
    "mode_gender = mode_gender.select('sex').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "proper-catholic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:28:26.963138Z",
     "start_time": "2023-11-25T11:28:26.959391Z"
    },
    "id": "proper-catholic",
    "outputId": "31ee99d7-b520-47ba-cbd5-fe527ce029df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ready-brush",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:31:29.567226Z",
     "start_time": "2023-11-25T11:31:29.551865Z"
    },
    "id": "ready-brush"
   },
   "outputs": [],
   "source": [
    "## fillna\n",
    "df_names_imputed = df_names.fillna(value = mode_gender,subset=['sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "hungarian-gambling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:33:28.495350Z",
     "start_time": "2023-11-25T11:33:28.329460Z"
    },
    "id": "hungarian-gambling",
    "outputId": "9a0b3327-9c0c-47ea-d28d-bce50930750f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "df_names_imputed.filter(df_names_imputed.sex.isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-anchor",
   "metadata": {
    "id": "realistic-anchor"
   },
   "source": [
    "### Drop Null Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-cleaners",
   "metadata": {
    "id": "swedish-cleaners"
   },
   "source": [
    "Drop rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-caribbean",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:35:14.011490Z",
     "start_time": "2023-11-25T11:35:13.663937Z"
    },
    "id": "vertical-caribbean",
    "outputId": "eece13c1-efd0-45f4-ab03-972c00a01397"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1921922"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count after dropping, any row with one null entry\n",
    "df_names.dropna().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-control",
   "metadata": {
    "id": "substantial-control"
   },
   "source": [
    "`dropna` accepts the following arguments\n",
    "\n",
    "- `how`: can be `'any'` or `'all'`. If `'any'`, rows containing any null values will be dropped entirely (this is the default). If `'all'`, only rows which are entirely empty will be dropped.\n",
    "\n",
    "- `thresh`: accepts an integer representing the \"threshold\" for how many empty cells a row must have before being dropped. `tresh` is a middle ground between `how='any'` and `how='all'`. As a result, the presence of `thresh` will override `how`\n",
    "\n",
    "- `subset`: accepts a list of column names. When a subset is present, N/A values will only be checked against the columns whose names are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-crazy",
   "metadata": {
    "id": "desirable-crazy"
   },
   "source": [
    "## Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "heated-priority",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:38:21.255166Z",
     "start_time": "2023-11-25T11:38:18.271819Z"
    },
    "id": "heated-priority",
    "outputId": "cb583e15-3718-45cf-cf5d-ca954eee08b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "1924581"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names.dropDuplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb035b7a-167f-4364-bd91-e24eb0c54feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d557fe2-57d7-483d-b3d9-ce443e4328fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "processed-medline",
   "metadata": {
    "id": "processed-medline"
   },
   "source": [
    "You can also pass a subset as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-storm",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:38:45.416363Z",
     "start_time": "2023-11-25T11:38:43.455630Z"
    },
    "id": "industrial-storm",
    "outputId": "9a53b6d4-e201-4481-84cd-eada837a2997",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048444"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names.dropDuplicates(['name','sex','n']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-hands",
   "metadata": {
    "id": "functioning-hands"
   },
   "source": [
    "## Save the Clean DataFrame to a New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "118b79ac-07e1-429e-97a7-48c6d0cd30be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_names_imputed_8.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54d3232d-6963-42dd-b972-02350d0bd3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names_imputed_8 = df_names_imputed.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff315f9e-cbb0-4ad4-8345-21dff7ce2380",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fantastic-jimmy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:41:11.900478Z",
     "start_time": "2023-11-25T11:41:11.295703Z"
    },
    "id": "fantastic-jimmy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_names_imputed_8.write.csv(\"hdfs://namenode-master/baby_names_postlab_byy_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-purchase",
   "metadata": {
    "id": "foreign-purchase"
   },
   "source": [
    "Notice that after writing to parquet it doesnt create 1 parquet file but multiple ones. that is because each partiton of the df is written as a parquet file. We will discuss partitioning in more detail in the next lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-warehouse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-25T11:44:14.038863Z",
     "start_time": "2023-11-25T11:44:12.911080Z"
    },
    "id": "stable-warehouse"
   },
   "outputs": [],
   "source": [
    "## stop the driver\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-preview",
   "metadata": {
    "id": "sublime-preview"
   },
   "source": [
    "## Task - Cleaning and encoding green taxis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-freedom",
   "metadata": {
    "id": "medical-freedom"
   },
   "source": [
    "- read the parquet file. tip: viewing the df can be very ugly if there are many features. you can choose the option vertical=TRUE to view each record as a column rather than a row.(`df.show(vertical=True)`)\n",
    "- For columns 'ehail_fee' and 'congestion_surcharge' replace nulls with 0.\n",
    "- For 'payment_type' replace the missing values the mode.\n",
    "- Create a new column called `distance_km` that calculates the distance in km. It is currenlty in miles. 1 mile = 1.61 km.\n",
    "- save the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-playback",
   "metadata": {
    "id": "orange-playback"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
